{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d39d4e-9145-4df4-8130-ef02c435dae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"magnusp_catalog\")\n",
    "dbutils.widgets.text(\"volume_path\", \"/Volumes/magnusp_catalog/training/source\")\n",
    "dbutils.widgets.text(\"target_schema\", \"training_raw\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f444400-e93c-4ef2-bcb6-5ca18c6574a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# clear cache so that if we run multiple times to check performance,\n",
    "\n",
    "\n",
    "UNIQUE_PLANS = 20\n",
    "PLAN_MIN_VALUE = 100\n",
    "\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 1\n",
    "data_rows = UNIQUE_PLANS # we'll generate one row for each plan\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "\n",
    "plan_dataspec = (\n",
    "    dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested)\n",
    "    .withColumn(\"plan_id\",\"int\", minValue=PLAN_MIN_VALUE, uniqueValues=UNIQUE_PLANS)\n",
    "    # use plan_id as root value\n",
    "    .withColumn(\"plan_name\", prefix=\"plan\", baseColumn=\"plan_id\")\n",
    "\n",
    "    # note default step is 1 so you must specify a step for small number ranges,\n",
    "    .withColumn(\"cost_per_mb\", \"decimal(5,3)\", minValue=0.005, maxValue=0.050,\n",
    "                step=0.005, random=True)\n",
    "    .withColumn(\"cost_per_message\", \"decimal(5,3)\", minValue=0.001, maxValue=0.02,\n",
    "                step=0.001, random=True)\n",
    "    .withColumn(\"cost_per_minute\", \"decimal(5,3)\", minValue=0.001, maxValue=0.01,\n",
    "                step=0.001, random=True)\n",
    "\n",
    "    # we're modelling long distance and international prices simplistically -\n",
    "    # each is a multiplier thats applied to base rate\n",
    "    .withColumn(\"ld_multiplier\", \"decimal(5,3)\", minValue=1.5, maxValue=3, step=0.05,\n",
    "                random=True, distribution=\"normal\", omit=True)\n",
    "    .withColumn(\"ld_cost_per_minute\", \"decimal(5,3)\",\n",
    "                expr=\"cost_per_minute * ld_multiplier\",\n",
    "                baseColumns=['cost_per_minute', 'ld_multiplier'])\n",
    "    .withColumn(\"intl_multiplier\", \"decimal(5,3)\", minValue=2, maxValue=4, step=0.05,\n",
    "                random=True,  distribution=\"normal\", omit=True)\n",
    "    .withColumn(\"intl_cost_per_minute\", \"decimal(5,3)\",\n",
    "                expr=\"cost_per_minute * intl_multiplier\",\n",
    "                baseColumns=['cost_per_minute', 'intl_multiplier'])\n",
    "            )\n",
    "\n",
    "df_plans = plan_dataspec.build().cache()\n",
    "\n",
    "display(df_plans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38c70b66-e91c-4909-a35c-92aa544f7bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume_path = dbutils.widgets.get(\"volume_path\")\n",
    "if volume_path:\n",
    "    df_plans.write.format(\"json\").mode(\"overwrite\").save(f\"{volume_path}/plans.json\")\n",
    "else:\n",
    "    print(\"Could not get path from parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2763d5e9-7302-4fc8-8741-ca8bcc83e8ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "UNIQUE_CUSTOMERS = 50000\n",
    "CUSTOMER_MIN_VALUE = 1000\n",
    "DEVICE_MIN_VALUE = 1000000000\n",
    "SUBSCRIBER_NUM_MIN_VALUE = 1000000000\n",
    "\n",
    "spark.catalog.clearCache()  # clear cache so that if we run multiple times to check\n",
    "                            # performance, we're not relying on cache\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 8\n",
    "data_rows = UNIQUE_CUSTOMERS\n",
    "\n",
    "customer_dataspec = (dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested)\n",
    "            .withColumn(\"customer_id\",\"decimal(10)\", minValue=CUSTOMER_MIN_VALUE,\n",
    "                        uniqueValues=UNIQUE_CUSTOMERS)\n",
    "            .withColumn(\"customer_name\", template=r\"\\\\w \\\\w|\\\\w a. \\\\w\")\n",
    "\n",
    "            # use the following for a simple sequence\n",
    "            #.withColumn(\"device_id\",\"decimal(10)\", minValue=DEVICE_MIN_VALUE,\n",
    "            #              uniqueValues=UNIQUE_CUSTOMERS)\n",
    "\n",
    "            .withColumn(\"device_id\",\"decimal(10)\",  minValue=DEVICE_MIN_VALUE,\n",
    "                        baseColumn=\"customer_id\", baseColumnType=\"hash\")\n",
    "\n",
    "            .withColumn(\"phone_number\",\"decimal(10)\",  minValue=SUBSCRIBER_NUM_MIN_VALUE,\n",
    "                        baseColumn=[\"customer_id\", \"customer_name\"], baseColumnType=\"hash\")\n",
    "\n",
    "            # for email, we'll just use the formatted phone number\n",
    "            .withColumn(\"email\",\"string\",  format=\"subscriber_%s@myoperator.com\",\n",
    "                        baseColumn=\"phone_number\")\n",
    "            .withColumn(\"plan\", \"int\", minValue=PLAN_MIN_VALUE, uniqueValues=UNIQUE_PLANS,\n",
    "                        random=True)\n",
    "            )\n",
    "\n",
    "df_customers = (customer_dataspec.build()\n",
    "                .dropDuplicates([\"device_id\"])\n",
    "                .dropDuplicates([\"phone_number\"])\n",
    "                .orderBy(\"customer_id\")\n",
    "                .cache()\n",
    "               )\n",
    "\n",
    "effective_customers = df_customers.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a230cb8-cdb5-43ed-9cdc-d3817d960a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "volume_path = dbutils.widgets.get(\"volume_path\")\n",
    "if volume_path:\n",
    "    df_customers.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(f\"{volume_path}/customers.csv\")\n",
    "else:\n",
    "    print(\"Could not get path from parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f052b3b5-e147-43d0-8ecd-3c4ef69ecbf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "AVG_EVENTS_PER_CUSTOMER = 50\n",
    "\n",
    "spark.catalog.clearCache()\n",
    "shuffle_partitions_requested = 8\n",
    "partitions_requested = 8\n",
    "NUM_DAYS=31\n",
    "MB_100 = 100 * 1000 * 1000\n",
    "K_1 = 1000\n",
    "data_rows = AVG_EVENTS_PER_CUSTOMER * UNIQUE_CUSTOMERS * NUM_DAYS\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions_requested)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 20000)\n",
    "\n",
    "\n",
    "# use random seed method of 'hash_fieldname' for better spread - default in later builds\n",
    "events_dataspec = (dg.DataGenerator(spark, rows=data_rows, partitions=partitions_requested,\n",
    "                   randomSeed=42, randomSeedMethod=\"hash_fieldname\")\n",
    "             # use same logic as per customers dataset to ensure matching keys\n",
    "             # but make them random\n",
    "            .withColumn(\"device_id_base\",\"decimal(10)\", minValue=CUSTOMER_MIN_VALUE,\n",
    "                        uniqueValues=UNIQUE_CUSTOMERS,\n",
    "                        random=True, omit=True)\n",
    "            .withColumn(\"device_id\",\"decimal(10)\",  minValue=DEVICE_MIN_VALUE,\n",
    "                        baseColumn=\"device_id_base\", baseColumnType=\"hash\")\n",
    "\n",
    "            # use specific random seed to get better spread of values\n",
    "            .withColumn(\"event_type\",\"string\",\n",
    "                        values=[ \"sms\", \"internet\", \"local call\", \"ld call\", \"intl call\" ],\n",
    "                        weights=[50, 50, 20, 10, 5 ], random=True)\n",
    "\n",
    "            # use Gamma distribution for skew towards short calls\n",
    "            .withColumn(\"base_minutes\",\"decimal(7,2)\",\n",
    "                        minValue=1.0, maxValue=100.0, step=0.1,\n",
    "                        distribution=dg.distributions.Gamma(shape=1.5, scale=2.0),\n",
    "                        random=True, omit=True)\n",
    "\n",
    "            # use Gamma distribution for skew towards short transfers\n",
    "            .withColumn(\"base_bytes_transferred\",\"decimal(12)\",\n",
    "                        minValue=K_1, maxValue=MB_100,\n",
    "                        distribution=dg.distributions.Gamma(shape=0.75, scale=2.0),\n",
    "                        random=True, omit=True)\n",
    "\n",
    "            .withColumn(\"minutes\", \"decimal(7,2)\",\n",
    "                        baseColumn=[\"event_type\", \"base_minutes\"],\n",
    "                        expr= \"\"\"\n",
    "                              case when event_type in (\"local call\", \"ld call\", \"intl call\")\n",
    "                                  then base_minutes\n",
    "                                  else 0\n",
    "                              end\n",
    "                               \"\"\")\n",
    "            .withColumn(\"bytes_transferred\", \"decimal(12)\",\n",
    "                        baseColumn=[\"event_type\", \"base_bytes_transferred\"],\n",
    "                        expr= \"\"\"\n",
    "                              case when event_type = \"internet\"\n",
    "                                   then base_bytes_transferred\n",
    "                                   else 0\n",
    "                              end\n",
    "                               \"\"\")\n",
    "\n",
    "            .withColumn(\"event_ts\", \"timestamp\",\n",
    "                         data_range=dg.DateRange(\"2020-07-01 00:00:00\",\n",
    "                                                 \"2020-07-31 11:59:59\",\n",
    "                                                 \"seconds=1\"),\n",
    "                        random=True)\n",
    "\n",
    "            )\n",
    "\n",
    "df_events = events_dataspec.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17c3596d-913e-4c87-8f8d-6156c92375e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"target_schema\")\n",
    "if catalog_name and schema_name:\n",
    "    df_events.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.raw_events\")\n",
    "else:\n",
    "    print(\"Incomplete parameters\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "init",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
